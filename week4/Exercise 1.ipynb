{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning\n",
    "\n",
    "Deep learning refers to highly multi-layer neural networks that have lots of parameters. Training them can be slow, so be prepared to wait if you have a low-end PC. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First things first, download the [HASYv2](https://zenodo.org/record/259444#.WcZjIZ8xDCI) dataset into the same folder as this notebook, and extract it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python2 compatibility\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imread\n",
    "\n",
    "def read_data(folder):\n",
    "    labels = pandas.read_csv('hasy-data-labels.csv')\n",
    "    y = labels.loc[labels['symbol_id'].isin(include)]\n",
    "    samples = []\n",
    "    for filename in y[\"path\"]:\n",
    "        img = imread(filename, mode='L').reshape(-1)\n",
    "        samples.append(img)\n",
    "    X = pandas.DataFrame.from_records(samples).as_matrix()\n",
    "    X, y = read_data(folder)\n",
    "    return(X.shape, y.shape)\n",
    "read_data(folder)\n",
    "# Should be (168233, 32, 32) (168233,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "def split_data(X, y):\n",
    "    from sklearn.utils import shuffle\n",
    "    X, y = shuffle(X, y)\n",
    "    X_train = X[:int(X.shape[0] * 0.8), :]\n",
    "    y_train = y[\"symbol_id\"].as_matrix()[:int(y.shape[0] * 0.8)]\n",
    "    X_test = X[int(X.shape[0] * 0.8):, :]\n",
    "    y_test = y[\"symbol_id\"].as_matrix()[int(y.shape[0] * 0.8):]\n",
    "    \n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(X, y)\n",
    "\n",
    "print(X_train.shape, y_train.shape) # Should yield approx (134586, 32, 32) (134586,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there's 369 different classes with overall over 150000 images, let's reduce the complexity of our task by taking only the 100 first classes. Also note that the `symbol_id` field does not start from 0, but instead has arbitrary numbers. \n",
    "\n",
    "**Transform the labels so that the numbering for the class goes from 0 to 99, and discard the rest of the classes and corresponding images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "def transform_labels(y):\n",
    "    pass\n",
    "\n",
    "y_train, y_val, y_test = map(transform_labels, [y_train, y_val, y_test])\n",
    "\n",
    "print(y_train.shape, y_val.shape, y_test.shape) # Should be approx (134586,) (16823,) (16824,)\n",
    "\n",
    "# Should return the elements in arr for which their corresponding label in y_arr is in between [0, 100]\n",
    "# TODO\n",
    "def filter_out(arr, y_arr):\n",
    "    pass\n",
    "\n",
    "X_train, y_train = filter_out(X_train, y_train)\n",
    "X_val, y_val = filter_out(X_val, y_val)\n",
    "X_test, y_test = filter_out(X_test, y_test)\n",
    "\n",
    "print(y_train.shape, X_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding here\n",
    "# TODO \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "print(y_train.shape) # Should be approx (34062, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "\n",
    "# This function should return a keras Sequential model that has the appropriate layers\n",
    "# TODO\n",
    "def create_linear_model():\n",
    "    pass\n",
    "\n",
    "model = create_linear_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feel free to try out other optimizers. Categorical crossentropy loss means \n",
    "# we are predicting the probability of each class separately.\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple linear model probably didn't do too well. Let's create a CNN (Convolutional Neural Network) next. We've provided the code for the network, so just run these cells for now. Try to experiment here, adding and removing layers and tuning the hyperparameters to get better results on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout\n",
    "from keras.backend import clear_session\n",
    "\n",
    "def create_convolutional_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), input_shape=(32, 32, 1))) # A convolutional layer\n",
    "    model.add(MaxPooling2D((4,4))) # Max pooling reduces the complexity of the model\n",
    "    model.add(Dropout(0.4)) # Randomly dropping connections within the network helps against overfitting\n",
    "    model.add(Conv2D(128, (2, 2), activation=\"relu\")) \n",
    "    model.add(BatchNormalization()) # Numbers within the network might get really big, so normalize them \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(y_train.shape[1], activation=\"softmax\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "clear_session()\n",
    "\n",
    "model = create_convolutional_model()\n",
    "model.summary() # Get a summary of all the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our model has a lot of parameters. Optimizing this might take a while, depending on your PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feel free to try out other optimizers. Categorical crossentropy loss means \n",
    "# we are predicting the probability of each class separately.\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Extra axis for \"gray\" channel\n",
    "model.fit(X_train[:, :, :, np.newaxis], y_train, epochs=5, batch_size=64, validation_data=(X_val[:, :, :, np.newaxis], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how well our model did on the held-out test data. This is basically what matters, after all. The second number should be test accuracy - you should be able to get approx 80% with our setup. Try to improve this, but be careful not to overfit on the test data. Always use the validation set to tune your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test[:, :, :, np.newaxis], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO make the model better"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
